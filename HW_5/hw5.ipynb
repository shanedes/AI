{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOYBkbRbjGOIOMEFw21PVas",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanedes/AI/blob/master/HW_5/hw5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE3A9gpQI3xH",
        "colab_type": "text"
      },
      "source": [
        "#Category 1: General Concepts\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZXRZq1WiKpv",
        "colab_type": "text"
      },
      "source": [
        "###Artificial Intelligence\n",
        "\n",
        "* AI is essentially the an attempt at creating programs that can produce the same results as a the complex brain of a human.\n",
        "\n",
        "* Official Definition: A computer system able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between langauges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch4R7aQYiOlJ",
        "colab_type": "text"
      },
      "source": [
        "###Machine Learning\n",
        "* A program that dynamically develops itself based on data. Creating a model that can predictions based on a set of data.\n",
        "* A subset of Artificial Intelligence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWRrlO8ziRuh",
        "colab_type": "text"
      },
      "source": [
        "###Deep Learning\n",
        "* Generally refers to the use of Neural Networks, an implementation of machine learning that is based on the neuron stucture used in our brains.\n",
        "* A subset of machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDPrPoHW6kLX",
        "colab_type": "text"
      },
      "source": [
        "#Category 2: Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3X6nE5J15xQ",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression\n",
        "\n",
        "One of the simplest models that can be used in AI. It is essentially developing a straight line that fits the data the most. \n",
        "\n",
        "The general form for a linear regression model is this:\n",
        "\n",
        "$$ \\hat y = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n $$\n",
        "\n",
        "where $ \\hat y $ is the predicted output. $ b $ is the bias, $ x_1 $ is the first variable, and $ w_1 $ is the weight for $ x_1 $. This would go on for all $ n $ $x$'s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgMUTl9s4aMw",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Logistic Regression generally results in a value in the range of [0,1] where this used for classification, typically binary classication where there are only two outcomes.\n",
        "\n",
        "Logistic regression will use the sigmoid function for activation. This function can take in values to map them to a value in the range of (0,1) and has an S-shape when graphed.\n",
        "$$ \\sigma (z) = \\frac{1}{1 + e^{-z}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCxF_OQIQAPt",
        "colab_type": "text"
      },
      "source": [
        "*function to calculate the sigmoid function implemented in python*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL37ujGEQISw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(z * -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqmKrEmxIFUD",
        "colab_type": "text"
      },
      "source": [
        "###Gradients\n",
        "Mathematically, the gradient of a function is a vector that contains all of that function's partial derivatives.\n",
        "$$ \\triangledown f(t) = [ \\frac{\\delta f}{\\delta x_1}(t) , \\frac{\\delta f}{\\delta x_2}(t) , ...  \\frac{\\delta f}{\\delta x_n}(t)] $$\n",
        "\n",
        "Gradient helps to find the direction of greatest change, which helps you to find the steepest slope. This is used in statistical modeling, in order to optimize the iterative process of finding the minimum of a function by going in the direction of steepest descent. This process is called gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK5yIEaBIOkE",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Descent\n",
        "Gradient descent is a fairly efficient process that is used to find the minimum of a function by utilzing the gradient to find the steepest slope.\n",
        "\n",
        "The general process of Gradient Descent can be described as taking iterative steps based on the gradient and on the hyperparameter of the learning rate.\n",
        "This is how the size of steps are determined.\n",
        "$$ s = s - \\alpha \\triangledown  L$$\n",
        "where $\\alpha$ is the  learning rate and $\\triangledown L$ is the gradient. This iteratively produce steps of s when trying to find the minimum value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DjmymFlObtf",
        "colab_type": "text"
      },
      "source": [
        "*this is an example of logistic regression that also uses gradient descent*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLvA4R_LIPuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs\n",
        "epochs = 100\n",
        "\n",
        "# learning rate\n",
        "lr = .01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range(split):\n",
        "    z = np.dot(sgd_w, train_data[i]) + sgd_b \n",
        "    \n",
        "    # gradients for the different weights\n",
        "    # sig is also gradient for bias weight\n",
        "    sig = sigmoid(z) - train_label[i]\n",
        "    grad_w1 = sig * train_data[i][0]\n",
        "    grad_w2 = sig * train_data[i][1]\n",
        "\n",
        "    # applying learning rate on graidents\n",
        "    sgd_w[0] -= lr * grad_w1\n",
        "    sgd_w[1] -= lr * grad_w2\n",
        "    sgd_b -= lr * sig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3sreY2w4xe1",
        "colab_type": "text"
      },
      "source": [
        "# Category 3: Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxU3dIh7TPtd",
        "colab_type": "text"
      },
      "source": [
        "##CNN (ConvNet)\n",
        "\n",
        "CNNs or ConvNets are convolutional neural networks. CNNs are generally used in deep learning applications that deal with image classifications, because of the way it is able to extract features from pixels. CNNs are based on the process of convolution, which can be seen as a form of matrix filtering. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeQLFL4nTB9T",
        "colab_type": "text"
      },
      "source": [
        "*python implementation of a 2D convolution fuction*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUwBZ2fE2XUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def conv2d(input_mat, kernel_mat):\n",
        "\n",
        "  # kernel matrix size\n",
        "  k_size = kernel_mat.shape[0]\n",
        "\n",
        "  # input matrix size\n",
        "  i_size = input_mat.shape[0]\n",
        "\n",
        "  # convolution is impossible when kernel is larger than input\n",
        "  if (k_size > i_size):\n",
        "    raise Exception('Invalid: kernel matrix has greater size than input matrix')\n",
        "\n",
        "  out_size = i_size - k_size + 1\n",
        "\n",
        "  output_mat = np.zeros((out_size, out_size))\n",
        "\n",
        "  # convolve operation\n",
        "  for i in range(out_size):\n",
        "    for j in range(out_size):\n",
        "      for k in range(k_size):\n",
        "        for l in range(k_size): \n",
        "          output_mat[i][j] += kernel_mat[k][l] * input_mat[i+k][j+l]\n",
        "\n",
        "  return output_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzZeOsjxYDOl",
        "colab_type": "text"
      },
      "source": [
        "###ReLU\n",
        "ReLU is one of the most basic activation functions in machine learning. It returns the max of 0 and the input value. It is commonly used in computer vision based neural networks, and will be used in CNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4iP_IvTWVVe",
        "colab_type": "text"
      },
      "source": [
        "###Pooling\n",
        "pooling is a process that is used to simplify computation in Convolution neural networks. It essentially combines neural clusters for an input layer and outputs only one neuron from those clusters for the output layer. Maxpooling keeps the maximum value.\n",
        "\n",
        "*python implementation of a maxpooling function in two dimensions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvFjR5FLXX5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def maxpooling2d(input_mat, s):\n",
        "  \n",
        "  i_size = input_mat.shape[0]\n",
        "\n",
        "  # if s is larger it raises an exception\n",
        "  if (s > i_size):\n",
        "    raise Exception('Invalid: s has greater size than input matrix')\n",
        "\n",
        "  out_size = int(i_size / s)\n",
        "\n",
        "  output_mat = np.zeros((out_size, out_size))\n",
        "\n",
        "  for i in range(out_size):\n",
        "    for j in range(out_size):\n",
        "      output_mat[i][j] = input_mat[i*s][j*s]\n",
        "      for k in range(s):\n",
        "        for l in range(s):\n",
        "          output_mat[i][j] = max(output_mat[i][j], input_mat[k+i*s][l+j*s])\n",
        "\n",
        "  return output_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF_GJd2XS_7o",
        "colab_type": "text"
      },
      "source": [
        "##General Layout of a CNN\n",
        "A CNN can consist of hidden convolutional, ReLU, and pooling layers. These layers will take an input layer (usually an image) and essentially produce a simplified filtered output.\n",
        "\n",
        "This simplified output from those hidden layers is then inputed into a densely connected network which produces an N dimensional vector where N is the number of classes that the program has to choose from. This densely connect network is the final classification step of the CNN.\n",
        "\n",
        "*implementation of a build of a CNN model in Python*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JezXRZ_oZnZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32,(5,5),activation=’relu’,input_shape=(28,28,1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (5, 5), activation=’relu’))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation=’softmax’))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovfuRy89IcW4",
        "colab_type": "text"
      },
      "source": [
        "# Category 4: Compiling a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBjissKNHYkW",
        "colab_type": "text"
      },
      "source": [
        "### Learning Rate\n",
        "\n",
        "The learning rate, often symbolized as the symbol $\\alpha$, is a hyperparameter that dictates how quickly a model adapts to a problem. The learning rate tends to be a very small value in the range of (0,1). The smaller the learning rate the smaller the step sizes, this will often require more training the make up for the step sizes. A larger learning rate can take less training but will often arrive at an incorrect solution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md6Sp4TwOnlo",
        "colab_type": "text"
      },
      "source": [
        "### Epoch\n",
        "\n",
        "The number of epochs is another configurable hyperparamter that determines the number of times you will iterate with the training data to build up the model. More epochs will generally result in a more accurate model, but with a significant increase in time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef6TFec7bVF4",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function\n",
        "Loss is essentially the penalty that is given an incorrect prediction is made by the model. Loss is the quantity that will be minimzed during training. The choice of a loss function will largely impact how the model develops over the course of training. There are several loss functions that are appropriate based on the type of problem that is being dealt with. \n",
        "\n",
        "For Example,\n",
        "\n",
        "Binary Classification  tends to have binary_crossentropy as the loss function.\n",
        "\n",
        "Multiclass Classification tends to have categorical_crossentropy.\n",
        "\n",
        "Regression tends to have mean squared error.\n",
        "\n",
        "*binary_crossentropy  function implementation in Python* \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0jjz67Ueux-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_cross_entropy_loss(x, y):\n",
        "  return -1 * x * np.log10(y) - (1-x) * np.log10(1-y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQMuMDxRaoFQ",
        "colab_type": "text"
      },
      "source": [
        "### Optimizers\n",
        "\n",
        "Optimizers determines how the loss function will affect the model during training. The steps in gradient descent is an example of this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfghIt5xh2z_",
        "colab_type": "text"
      },
      "source": [
        "*how a model is compiled in Keras*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCqqjMrwh8To",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy', \n",
        "    optimizer=optimizers.RMSprop(lr=2e-5), \n",
        "    metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddnx5mFK1xp",
        "colab_type": "text"
      },
      "source": [
        "#Catergory 5: Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjArLYgAjRmM",
        "colab_type": "text"
      },
      "source": [
        "###Training\n",
        "training is generally going through iteratively through the training data set and adjusting the weights and bias of the model based on the loss function and the learning rate, where loss is trying to be minimized. The number of iterations through the data set depends on the number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NJpVQOHnl51",
        "colab_type": "text"
      },
      "source": [
        "*training a model in Python*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKY0FXMLnkDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD1tsIAckexF",
        "colab_type": "text"
      },
      "source": [
        "###Overfitting\n",
        "Overfitting happens when the model develops to be too complex for the general problem. This is often the result of having too much training choosing model that is far more complex than the scope of the problem. When you train too much on a data set the model will develop around all the outliers and any blemish in the data, so new data will not actually be predicted how it is supposed to be. One could reduce the risk for overfitting by trying to reduce the noise in the data set, which would make the model more able to extroplate for future data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LraNmZWl52f",
        "colab_type": "text"
      },
      "source": [
        "###Underfitting\n",
        "Underfitting is the complete oppisite issue of overfitting. Underfitting happens when the model develops to be too simple for the general problem. This is often the result of sparse data or not enough training, but it could also be the selection of a model type that is too simple for the problem at hand. This can generally be improved by gathering more data and training the model more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yopBRiRKm0R6",
        "colab_type": "text"
      },
      "source": [
        "###Testing\n",
        "although the quality of a model is often determined by the amound of data trained on, you will also want to reserve some of the data collected for testing purposes to see how the model is able to extrapolate for new data points. This testing data will allow someone to see if the data is underfitting or overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9UrOWmJQAO7",
        "colab_type": "text"
      },
      "source": [
        "#Category 6: Finetuning a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxv1stOhotqU",
        "colab_type": "text"
      },
      "source": [
        "###Pretrained model\n",
        "A pretrained model is a common approach to deep learning on small datasets. \n",
        "It is a network, usually publicly available, that has already been  trained on a large benchmark dataset that solves a similar problem to the one that our intended model will want to solve. Using these models is an efficent way to save time, since training models is one of the most time-consuming aspects of developing deep learning models, so pretrained models save a lot of time and computation.\n",
        "\n",
        "Some known pretrained models are VGG, Inception , and MobileNet etc. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgD0S3pNv_b9",
        "colab_type": "text"
      },
      "source": [
        "*Here I load a pre-trained model Xception so I can use it's convolutional base*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0FgtANCXm_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import Xception\n",
        "\n",
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwdan9_OqHEc",
        "colab_type": "text"
      },
      "source": [
        "###Freezing\n",
        "When you \"freeze\" a layer that means you are not updating its appropriate weights during training. We would often want to freeze the lower/earlier layers of the model, since those layers tend to be independent of the specificic problem. Freezing layers is often good to reduce computation costs and to also prevent overfitting on small datasets, since lower layers have already been trained on large datasets beforehand for most pre-trained models.\n",
        "\n",
        "You can also freeze the entire convolutional base. This means that you are maintaining the base layers of the CNN, and using it exclusively to extract features as-is to the classifier network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNe6X-PJwQLR",
        "colab_type": "text"
      },
      "source": [
        "*here I freeze the entire convolutional base from the pretrained model from above*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UleRo4Dpq6Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6s5eIlXtdAC",
        "colab_type": "text"
      },
      "source": [
        "###Finetuning\n",
        "In general finetuning is when you add layers to a pretrained model and the freeze some lower layers that you think should be maintained the same for your problem and data at hand. This can vary from freezing a few of the first relevant layers to freezing the whole convolutional base. Again, this decision depends on how much you data fits with the data that the pretrained model was trained on and how much data you have."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8etYh7vnli",
        "colab_type": "text"
      },
      "source": [
        "*This is an example of unfreezing some layers in the convolutional base, so we can fine-tune them*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLK9Z8WDQnFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'conv2d_3':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}